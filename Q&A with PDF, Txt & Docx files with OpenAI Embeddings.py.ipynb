{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26998722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query (type 'exit' to end): what is the aim of the project?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: The aim of the project is to develop and implement advanced language translation technology to facilitate communication between speakers of low-resource tribal languages and speakers of other languages, and to address the issue of many tribal languages becoming extinct due to limited resources for language preservation.\n",
      "Source: \Q&A with PDF & Txt\\Documents\\Low Resource Language Translation.pptx\n",
      "Page: \n",
      "\n",
      "Enter your query (type 'exit' to end): how many languages has OCR?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Tesseract OCR contains Ol Chiki (Santali) and Meetei Meyek (Manipuri) scripts.\n",
      "Source: \Q&A with PDF & Txt\\Documents\\ChatGPT and DALL-E for Text to Image conversion.pptx\n",
      "Page: \n",
      "\n",
      "Enter your query (type 'exit' to end): what is the range of ford lightning?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: The Ford Lightning has a range of over 300 miles on a single charge.\n",
      "Source: \Q&A with PDF & Txt\\Documents\\All_News_with_NER_RE.txt\n",
      "Page: \n",
      "\n",
      "Enter your query (type 'exit' to end): Who's the editor in chief?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Mag. Matthias Wasinger, MOS PhD\n",
      "Source: C:\\Users\\nithi\\Downloads\\Answergenie\\Q&A with PDF & Txt\\Documents\\Data Scientist.docx\n",
      "Page: \n",
      "\n",
      "Enter your query (type 'exit' to end): on which date you applied the model to predict theft incidents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: January 25th, 2014 to January 31st, 2014\n",
      "Source: \Q&A with PDF & Txt\\Documents\\2015_Crime_prediction_using_twitter_sentiment_and_weather_11.pdf\n",
      "Page: 4\n",
      "\n",
      "Enter your query (type 'exit' to end): example of exaggerated forms are\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: I don't know.\n",
      "Source: \Q&A with PDF & Txt\\Documents\\2015_Crime_prediction_using_twitter_sentiment_and_weather_11.pdf\n",
      "Page: 4\n",
      "\n",
      "Enter your query (type 'exit' to end): what is a metaphor?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: A metaphor is a figure of speech that provides a language technique for conveying thoughts and notions that are not the same as they appear on the surface, it directly refers to one thing by mentioning another, and to efficiently identify its hidden sentiment, the model must be capable of reading between the lines.\n",
      "Source: \Q&A with PDF & Txt\\Documents\\2022_Crime_prediction_using_a_hybrid_sentiment_analysis_approach_10.pdf\n",
      "Page: 7\n",
      "\n",
      "Enter your query (type 'exit' to end): exit\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain import OpenAI, VectorDBQA\n",
    "import pickle\n",
    "import textwrap\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader, TextLoader, UnstructuredWordDocumentLoader, UnstructuredPowerPointLoader\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set up the environment variable for the OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "\n",
    "def get_documents(file_paths):\n",
    "    documents = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        _, extension = os.path.splitext(file_path.lower())\n",
    "        if '.docx' in extension or '.doc' in extension:\n",
    "            docx_loader = UnstructuredWordDocumentLoader(file_path)\n",
    "            docx_document = docx_loader.load()\n",
    "            documents += docx_document\n",
    "        elif '.pdf' in extension:\n",
    "            pdf_loader = PyPDFLoader(file_path)\n",
    "            pdf_document = pdf_loader.load()\n",
    "            documents += pdf_document\n",
    "        elif '.txt' in extension:\n",
    "            txt_loader = TextLoader(file_path)\n",
    "            txt_document = txt_loader.load()\n",
    "            documents += txt_document\n",
    "        elif '.pptx' in extension:\n",
    "            pptx_loader = UnstructuredPowerPointLoader(file_path)\n",
    "            pptx_document = pptx_loader.load()\n",
    "            documents += pptx_document\n",
    "        elif '.docx' in extension or '.doc' in extension and '.pdf' in extension:\n",
    "            docx_loader = UnstructuredWordDocumentLoader(file_path)\n",
    "            docx_document = docx_loader.load()\n",
    "            documents += docx_document\n",
    "            pdf_loader = PyPDFLoader(file_path)\n",
    "            pdf_document = pdf_loader.load()\n",
    "            documents += pdf_document\n",
    "        elif '.docx' in extension or '.doc' in extension and '.txt' in extension:\n",
    "            docx_loader = UnstructuredWordDocumentLoader(file_path)\n",
    "            docx_document = docx_loader.load()\n",
    "            documents += docx_document\n",
    "            txt_loader = TextLoader(file_path)\n",
    "            txt_document = txt_loader.load()\n",
    "            documents += txt_document\n",
    "        elif '.docx' in extension or '.doc' in extension and '.pptx' in extension:\n",
    "            docx_loader = UnstructuredWordDocumentLoader(file_path)\n",
    "            docx_document = docx_loader.load()\n",
    "            documents += docx_document\n",
    "            pptx_loader = UnstructuredPowerPointLoader(file_path)\n",
    "            pptx_document = pptx_loader.load()\n",
    "        elif '.pdf' in extension and '.txt' in extension:\n",
    "            pdf_loader = PyPDFLoader(file_path)\n",
    "            pdf_document = pdf_loader.load()\n",
    "            documents += pdf_document\n",
    "            txt_loader = TextLoader(file_path)\n",
    "            txt_document = txt_loader.load()\n",
    "            documents += txt_document\n",
    "        elif '.pdf' in extension and '.pptx' in extension:\n",
    "            pdf_loader = PyPDFLoader(file_path)\n",
    "            pdf_document = pdf_loader.load()\n",
    "            documents += pdf_document\n",
    "            pptx_loader = UnstructuredPowerPointLoader(file_path)\n",
    "            pptx_document = pptx_loader.load()\n",
    "            documents += pptx_document\n",
    "        elif '.pdf' in extension and '.docx' in extension or '.doc' in extension:\n",
    "            pdf_loader = PyPDFLoader(file_path)\n",
    "            pdf_document = pdf_loader.load()\n",
    "            documents += pdf_document\n",
    "            docx_loader = UnstructuredWordDocumentLoader(file_path)\n",
    "            docx_document = docx_loader.load()\n",
    "            documents += docx_document\n",
    "        elif '.txt' in extension and '.pptx' in extension:\n",
    "            txt_loader = TextLoader(file_path)\n",
    "            txt_document = txt_loader.load()\n",
    "            documents += txt_document\n",
    "            pptx_loader = UnstructuredPowerPointLoader(file_path)\n",
    "            pptx_document = pptx_loader.load()\n",
    "            documents += pptx_document\n",
    "        elif '.pdf' in extension and '.docx' in extension or '.doc' in extension and'.pptx' in extension:\n",
    "            pdf_loader = PyPDFLoader(file_path)\n",
    "            pdf_document = pdf_loader.load()\n",
    "            documents += pdf_document\n",
    "            docx_loader = UnstructuredWordDocumentLoader(file_path)\n",
    "            docx_document = docx_loader.load()\n",
    "            documents += docx_document\n",
    "            pptx_loader = UnstructuredPowerPointLoader(file_path)\n",
    "            pptx_document = pptx_loader.load()\n",
    "            documents += pptx_document\n",
    "        elif '.txt' in extension and '.docx' in extension or '.doc' in extension and'.pptx' in extension:\n",
    "            txt_loader = TextLoader(file_path)\n",
    "            txt_document = txt_loader.load()\n",
    "            documents += txt_document\n",
    "            docx_loader = UnstructuredWordDocumentLoader(file_path)\n",
    "            docx_document = docx_loader.load()\n",
    "            documents += docx_document\n",
    "            pptx_loader = UnstructuredPowerPointLoader(file_path)\n",
    "            pptx_document = pptx_loader.load()\n",
    "            documents += pptx_document\n",
    "        elif '.txt' in extension and '.docx' in extension or '.doc' in extension and'.pdf' in extension:\n",
    "            txt_loader = TextLoader(file_path)\n",
    "            txt_document = txt_loader.load()\n",
    "            documents += txt_document\n",
    "            docx_loader = UnstructuredWordDocumentLoader(file_path)\n",
    "            docx_document = docx_loader.load()\n",
    "            documents += docx_document\n",
    "            pdf_loader = PyPDFLoader(file_path)\n",
    "            pdf_document = pdf_loader.load()\n",
    "            documents += pdf_document\n",
    "        elif '.txt' in extension and '.pptx' in extension and'.pdf' in extension:\n",
    "            txt_loader = TextLoader(file_path)\n",
    "            txt_document = txt_loader.load()\n",
    "            documents += txt_document\n",
    "            pptx_loader = UnstructuredPowerPointLoader(file_path)\n",
    "            pptx_document = pptx_loader.load()\n",
    "            documents += pptx_document\n",
    "            pdf_loader = PyPDFLoader(file_path)\n",
    "            pdf_document = pdf_loader.load()\n",
    "            documents += pdf_document\n",
    "        elif '.docx' in extension or '.doc' in extension and '.pdf' in extension and '.txt' in extension and '.pptx' in extenstion:\n",
    "            docx_loader = UnstructuredWordDocumentLoader(file_path)\n",
    "            docx_document = docx_loader.load()\n",
    "            documents += docx_document\n",
    "            pdf_loader = PyPDFLoader(file_path)\n",
    "            pdf_document = pdf_loader.load()\n",
    "            documents += pdf_document\n",
    "            txt_loader = TextLoader(file_path)\n",
    "            txt_document = txt_loader.load()\n",
    "            documents += txt_document\n",
    "            pptx_loader = UnstructuredPowerPointLoader(file_path)\n",
    "            pptx_document = pptx_loader.load()\n",
    "            documents += pptx_document\n",
    "\n",
    "    return documents\n",
    "\n",
    "def get_query_result(query, documents):\n",
    "    # Split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "\n",
    "    # Query documents\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=os.environ['OPENAI_API_KEY'])\n",
    "    docsearch = Chroma.from_documents(texts, embeddings)\n",
    "    qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=docsearch, return_source_documents=True)\n",
    "    result = qa({\"query\": query})\n",
    "\n",
    "    result_text = result['result'].strip()\n",
    "    source = result.get('source_documents', [{}])[0].metadata.get('source', '')\n",
    "    page = result.get('source_documents', [{}])[0].metadata.get('page', '')\n",
    "\n",
    "    return result_text, source, page\n",
    "\n",
    "def chat_loop(file_paths):\n",
    "    documents = get_documents(file_paths)\n",
    "    if not documents:\n",
    "        print(\"No supported files found.\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        query = input(\"Enter your query (type 'exit' to end): \")\n",
    "        if query.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        result = get_query_result(query, documents)\n",
    "\n",
    "        if result is not None:\n",
    "            result_text, source, page = result\n",
    "            print(\"Result:\", result_text)\n",
    "            if source:\n",
    "                print(\"Source:\", source)\n",
    "                print(\"Page:\", page)\n",
    "        else:\n",
    "            print(\"No answer found for the query.\")\n",
    "\n",
    "        print()  # Print an empty line for separation\n",
    "\n",
    "# Get the file paths from the webpage or wherever you have them\n",
    "file_paths = [\n",
    "    r'C:\\Users\\nithi\\Downloads\\Answergenie\\Q&A with PDF & Txt\\Documents\\2022_Crime_prediction_using_a_hybrid_sentiment_analysis_approach_10.pdf',\n",
    "    r'C:\\Users\\nithi\\Downloads\\Answergenie\\Q&A with PDF & Txt\\Documents\\2015_Crime_prediction_using_twitter_sentiment_and_weather_11.pdf',\n",
    "    r'C:\\Users\\nithi\\Downloads\\Answergenie\\Q&A with PDF & Txt\\Documents\\Low Resource Language Translation.pptx',\n",
    "    r'C:\\Users\\nithi\\Downloads\\Answergenie\\Q&A with PDF & Txt\\Documents\\All_News_with_NER_RE.txt',\n",
    "    r'C:\\Users\\nithi\\Downloads\\Answergenie\\Q&A with PDF & Txt\\Documents\\Invitation_Amit Kumar.docx'\n",
    "]\n",
    "\n",
    "# Start the chat loop\n",
    "chat_loop(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236bb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
